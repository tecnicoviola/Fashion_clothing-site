# -*- coding: utf-8 -*-
"""Search_cosine_similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-ROMrzb5YZ3DCP5hcBkscAA0jmXz5Ymn
"""
import random
import pandas as pd
import numpy as np
import re
import operator
import nltk 
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import defaultdict
from nltk.corpus import wordnet as wn
from sklearn.feature_extraction.text import TfidfVectorizer

# nltk.download('punkt')
# nltk.download('wordnet')
# nltk.download('averaged_perceptron_tagger')
# nltk.download('stopwords')
  
# df = pd.read_csv('/content/Myntra_dataset.csv')
df = pd.read_csv('Myntra_dataset.csv')

df.sample(5)

df['total_info'] = df['dominant_color'] + str(' ') +	df['product_type'] + str(' size ') + df['size']

df['total_info'] = df['total_info'].apply(lambda x: str(x).translate(str.maketrans('', '', string.punctuation)).lower())

product = list(df['total_info']) # array

from sklearn.feature_extraction.text import TfidfVectorizer
import operator
## Create Vocabulary
vocabulary = set()
for doc in product:
    vocabulary.update(doc.split(' '))
vocabulary = list(vocabulary)
# Intializating the tfIdf model
tfidf = TfidfVectorizer(vocabulary=vocabulary) # forms vocabulary and vector of very word
# Fit the TfIdf model 
tfidf.fit(product)
# Transform the TfIdf model
tfidf_tran=tfidf.transform(product)

def gen_vector_T(tokens):
  Q = np.zeros((len(vocabulary)))    
  x= tfidf.transform(tokens)
  #print(tokens[0].split(','))
  for token in tokens[0].split(','):
      #print(token)
      try:
          ind = vocabulary.index(token)
          Q[ind]  = x[0, tfidf.vocabulary_[token]]
      except:
          pass
  return Q

def cosine_sim(a, b): 
    cos_sim = np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))
    return cos_sim

def cosine_similarity_T(k, query):
    preprocessed_query = re.sub("\W+", " ", query.lower()).strip()
    preprocessed_query = ' '.join([word for word in preprocessed_query.split(' ')[2:]])
    tokens = word_tokenize(str(preprocessed_query))
    # q_df = pd.DataFrame(columns=['q_clean'])
    # q_df.loc[0,'q_clean'] =tokens
    d_cosines = []
    
    query_vector = gen_vector_T(tokens)
    for d in tfidf_tran.A:
        d_cosines.append(cosine_sim(query_vector, d))
                    
    out = np.array(d_cosines).argsort()[-k:][::-1]
    d_cosines.sort()
    a = pd.DataFrame()
    for i,index in enumerate(out):
        a.loc[i,'unique_id'] = df['uniq_id'][index]
        a.loc[i,'product_id'] = df['product_id'][index]
        a.loc[i,'title'] = df['title'][index]
        # a.loc[i,'images'] = df['images'][index]
        a.loc[i,'images'] = df['images'][index].split()[0]
        a.loc[i, 'dominant_color'] = df['dominant_color'][index]
        a.loc[i,'size'] = df['size'][index]
        a.loc[i, 'price'] = random.randint(500, 5000)
        
        a.loc[i,'product_type'] = df['product_type'][index]
    for j,simScore in enumerate(d_cosines[-k:][::-1]):
        a.loc[j,'Score'] = simScore
    return a